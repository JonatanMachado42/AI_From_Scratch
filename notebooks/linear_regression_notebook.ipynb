{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33cd3b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.9702924142333487\n",
      "-------\n",
      "[-12.47981109   1.29268946 -20.98310145  -9.52456324  -0.74307416]\n"
     ]
    }
   ],
   "source": [
    "# Data creation \n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "root = os.path.abspath(\"..\")  # sobe de notebooks → raiz\n",
    "sys.path.append(root)\n",
    "\n",
    "from src.preprocessing.scalers import MinMaxScaler\n",
    "\n",
    "def generate_linear_regression_data(n_samples=200, noise_std=1.0):\n",
    "    # Relação linear verdadeira: y = 3*x + 5\n",
    "    X = np.random.uniform(-10, 10, size=(n_samples, 1))\n",
    "    y = 3 * X[:, 0] + 5 + np.random.normal(0, noise_std, size=n_samples)\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_linear_regression_data()\n",
    "print(X[1][0])\n",
    "print(\"-------\")\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7396bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21739136]\n",
      " [0.40110735]\n",
      " [0.0661648 ]\n",
      " [0.25505081]\n",
      " [0.39697613]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X)\n",
    "normalized_dataset = scaler.transform(X)\n",
    "\n",
    "print(normalized_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07019c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'W' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m([\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m W,b \n\u001b[0;32m---> 36\u001b[0m W, b \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m w \u001b[38;5;241m=\u001b[39m W[\u001b[38;5;241m0\u001b[39m]      \u001b[38;5;66;03m# peso escalar\u001b[39;00m\n\u001b[1;32m     39\u001b[0m b \u001b[38;5;241m=\u001b[39m b[\u001b[38;5;241m0\u001b[39m]      \u001b[38;5;66;03m# bias escalar\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m, in \u001b[0;36mlinear_regression\u001b[0;34m(X, y, alpha)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlinear_regression\u001b[39m(X,y, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m): \n\u001b[0;32m---> 25\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand([\u001b[43mW\u001b[49m[\u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m     26\u001b[0m     b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1500\u001b[39m):\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'W' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def linear_func(X,W,b):\n",
    "    return np.sum(X * W) + b\n",
    "\n",
    "def error(X, y, W, b):\n",
    "    return np.sum((linear_func(X,W,b)) - y )**2 / len(y)\n",
    "\n",
    "\n",
    "def gradient_error_weight(X,y,W,b): \n",
    "    return 2 * np.sum(((linear_func(X,W,b)) - y )*X) / len(y) \n",
    "\n",
    "def gradient_error_bias(X,y,W,b): \n",
    "    return 2 * np.sum(((linear_func(X,W,b)) - y )) / len(y) \n",
    "\n",
    "def gradient_descent(X,y,W,b, alpha):\n",
    "\n",
    "    W_new = W - alpha * gradient_error_weight(X,y,W,b)\n",
    "    b_new = b - alpha * gradient_error_bias(X,y,W,b)\n",
    "\n",
    "    W = W_new \n",
    "    b = b_new \n",
    "\n",
    "    return W, b \n",
    "\n",
    "def linear_regression(X,y, alpha = 0.001): \n",
    "    W = np.random.rand([W[0]])\n",
    "    b = np.random.rand(1)\n",
    "    \n",
    "    for i in range(1500):\n",
    "        W, b = gradient_descent(X,y, W, b, alpha)\n",
    "        print(len([0]))\n",
    "\n",
    "    return W,b \n",
    "\n",
    "\n",
    "\n",
    "W, b = linear_regression(X,y,5)\n",
    "\n",
    "w = W[0]      # peso escalar\n",
    "b = b[0]      # bias escalar\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = w * x + b\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
